---
title: "Lost In Translation"
description: |
  The loss between prediction and true outcome is not the only loss that occurs. Any machine learning model starts with a goal an institution or individual wants to solve, translated into a tool or process that helps with that goal, which again is translated into a prediction task, which is translated to data. This blog post proposes a mental tool to think on a high level about translating problems into prediction tasks.
author:
  - name: Christoph Molnar
date: 04-15-2019
slug: loss
output:
  radix::radix_article:
    self_contained: false
preview: images/predictive-policing.png
creative_commons: CC BY-NC
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Predictive models solve problems.
As a data scientist, after you got the data, and do the modeling, it is easy to forget the whole pipeline that led you there, and only focus on the loss ocurring between features and target.

Loss occurs way earlier.
Because before you dive into data, you have to translate you initial goal in multiple steps until you get to the data.


First there is some goal you have as a company, individual, institution or whatever.
The goal is the thing you want to achieve overall.
This is very high level and has nothing to do with machine learning or data.
If you are YouTube, a goal might be to increase the minutes watched per day.
Another goal could be to get more content creators onto the platform.

First translations: From goal to a process or tool to achieve the goal.
When you settled on a goal, you have to decide to achieve it. 

In the remainder of the post we will go through a few examples how this translation steps look like with three examples: customer relationship management, predictive policing and credit scoring.

Loss will always occur.
It's easy for me here to do the critique.
When you formulate problems as prediction tasks, you always have to make compromises, settle with the not so perfect solution.
Which is easy to critique.
The most important is that you have thought about the translations that have been made and how it affects the applicability of the predictions to the real world.
My advice: Make it explicit.
This blog post will help you with a few examples how to make those steps more explicit. 
Gives you a very rough template to do it (Goal -> Tool -> Prediction Task -> Data).


Another way to see it:
We start with a very broad goal, and end up with very specific, detailed data, where we have to decide which rows and columns to keep, what to use and so on.
We start with very broad intentions and narrow down the task until it is fit for machine learning.
With each narrowing, we are forced to specify our needs.
With each decision we loose something from our original goal.
We can even narrow down our goal so much that we end up with a wrong translation.


# Sending Vouchers to Customers

Imagine you manage a travel agency.
You sell trips to your customers, mostly to some very loyal customers and sometimes to new customers.
At the last trade show, the stand showing how you can sell more trips using data analytics and marketing had a great impression on you and you decide to hire them.

<img src='images/vouchers.jpg' alt='' title='Sending vouchers to customers to increase sales.'/>


**The Goal:** Selling more trips

The marketing consulting company advices you to send out vouchers to more customers.
With that translation of your goal to this specfic tool, the first loss in translation has occured:
Sending vouchers might not be the best tool.
Maybe uninvited vouchers will annoy your customers, maybe the low hanging fruits are a niche group of customers for whom you have the perfect trips, but they just don't know you yet.

But anyways, the decision has been made:

**The Tool:** Send vouchers to customers.


But wait!
Sending vouchers to everyone in your database would be too expensive.
Many customers are only file corpses in your drawers anyway.
The mailings cost more than zero (cause you decided that your customers will prefer good ol' physical paper mailings).
So, you need a way to send to "promising" / "high-value" customers.
We can translate this into a prediction task.
Why not predict which customer is most likely to book a trip in the next months.

**The prediction task:** Predict whether a customer will book a trip in the next 2 months.

The thinking is: If someone has a high likelihood do book, we want to make sure he books with us, and also the voucher might convince those that are not 100 percent sure whether to do a trip at all.

Another loss has occured by translating from the tool to the prediction task.
Maybe the customers who are likely to book will not be affected by the voucher, because they would have booked anyways.
You only loose money because they now book cheaper.

Now you have decided what to predict, the question is now which data to use.
You settle on past trips booked, demographic information, money spent on trips at your agency, time and date, ...
Ideally, you would have access to their bank account to see if they have enough money left, to their calenders to see if they have time, to their minds so you can read their thoughts.
But you only have your agencies database, so you use that.
Another translation has occured: From the prediction task to the data:

**The Data:** Customer demographics and prior trips.

A loss has ocurred as well.
This time you can measure it more directly, since you can measure how well the data predicts whether someone will book a trip in the next two weeks.
What's not easily measurable: The loss that ocurred by selecting a subset of the data, or by not using additional data.


# Predictive Policing

You are the head of the police department.
The executive arm of civil protection lies within your responsible hands.

<img src='images/predictive-policing.jpg' alt='' title='Working late to submit better predictions.'/>

**The Goal:** Keep the city safe.

Now, how exactly do you keep the city safe?
For example by catching criminals.
Even better, prevent crimes before they happen.
Also called "stop-and-frisk" in its extreme form.
So you translate the goal into:

**The Tool:** Prevent crimes by showing presence, doing stop-and-frisk.

A first loss has occurred:
Is this the most effective method?
Maybe the city could be made more secure by educating the people, by increasing the police staff to relief the burden of the many cases that are already on the table. 

Anyways, stop-and-frisk it is.
But your police staff can't be everywhere, you have to send them to specific areas.
Where to send them?
Send them to where you think most crime will happen.
This is a translation into a prediction task.

**Prediction Task:** For each place and time, predict likelihood of a crime.

A loss occurred: We could also predict who the most likely victims are or include the infromation which crimes are easily averted by presence and which not.

The data to use for those predictions is quite obvious: use spatial and time data of crimes that already ocurred in that time.
Additionally weather.

**The Data:**: Recent crimes, along with geolocation and time. Additional data: weather, occurrence of other events (e.g. soccer games).


Again, a loss occurred: We could have used completely different data sources that tell us more about the crime probabilities.
For example mobile location data or social media posting.
Also, by using the own crime data, there is a bias:
The closer the police looks, the more crimes it sees.
The other data might be dark:
Crimes that were never reported.
This loss is not reflected in our data, since we never expose our model to it, we only see the acurracy of predicting the crimes that we saw in the data.

# Credit Scoring

How can you trust someone to give you back money that you lend?
The answer in our society is credit scoring agencies.
In Germany the quasi-Monopoly is called the 'Schufa'.

<img src='images/schufa.jpg' alt='' title='Working late to submit better predictions.'/>

**Their goal:** Protect clients from credit risk.

How do they do it?

**Their tool:** Offer credit worthiness scores to lenders.

A loss occurred: Scores are not the only tool, they could also insure the credit.
They could allow lenders to deposit something.
They could assess customers individually (which, of course, would be a lot of work).

What does the score say?

**The prediction task:** Probability of loan default within 1 year (not sure if SCHUFA uses exactly this, but let's assume for now)

Sensible goal.
Still a loss occurred: Loan defaults after one year would be ignored.
From which data do we predict this?

**The Data:** Not revealed what data the SCHUFA uses, but data they have: current contracts with e.g. mobile providers, mail-order companies, credits that have not been paid and so on.

Loss in translation: Maybe a person never paid, e.g. because of an exploitve relationship, but this relationship is over.
This is not reflected in the database.
Also not in the data base: The people who don't get a loan in the first place, for whatever reason.
Those never show up in the database.
Self-reinforcing: People with a bad score don't get a loan.




# Lost in Translation

Every machine learning project has these decision that have to be made, which ultimately narrows down the question that can be answered.


With this post, I hope I could make it more explicit how we have to narrow down our original goal to a very specific slice of data.




# From Prediction to Action

Forgot to mention so far another, very important translation.
That is the translation back.

From your model, you get back predictions.
Those predictions don't spawn into the void to the pleasure of the universe, but rather inform decision and incite actions.
How that is done is crucial for the effectiviness of the model.

Back to YouTube: You have the predictions...

But this maybe for another blog post.


