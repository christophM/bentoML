
# ADM (Algorithmic Decision Making)

## about the document
- Idea: Create a post summarizing the document Technische und rechtliche Betrachtungen algorithmischer Entscheidungsverfahren
- Gutachten
- Create by  Fachgruppe Rechtsinformatik der Gesellschaft für Informatik e.V.
- Document was created for "Sachverständigenrats für Verbraucherbelange beim Bundesministerium der Justiz und für Verbraucherschutz"
- almost 200 pages long
- studies the (future) control of ADM systems from a legal and technical perspective
- focused on ADM affecting people (makes sense since Verbraucherbelange payed for the doc)


## Notes on content
- companies automate decisions, especially with machine learning, which can be unfair, or the models make mistakes, violate rights of others (p. 6)
- two methods to increase transparancy of adm tools: testing and auditing p. 7 ff.
- call to action: p.7
  - need for laws and legislation, but unclear how the framework should look like
  - need for standards for testing, auditing and certification of adm systems
  - doing tests of adm systems. tests should be kind of legally certified.
  - only when we have a good legal framework for tests, then the testing of adm systems should be legally required
  - companies should have to inform when they use such systems? to whom?
  - create a state owned institute for algorithmic, which controls the auditing. similar to Bundesamts für Sicherheit in der Informationstechnik (BSI)
  - universities have to educate especially in computer science and law school
  - needs research between computer science and law
- New legal questions are created by the use of the data, the learner and the model
- Dangers of ADMs p. 12: discrimination, mistakes
- They interviewed some people involved with ADM systems p. 19
- they explore some laws from other countries that partially cover ADM systems, e.g. fair lending from US.
- they show linear and logistic regression
- based on talking with practitioners they found logistic regression to be the most commonly method used for scoring. (data seems anecdotal, but matches my anecdots. e.g. schufa also uses logistic regression)
- discussing undersampling and oversampling for fairness. (i am not sure if that makes sense)
- excluding sensitive features might lead to the model relying on proxies.
- they discuss different measures of fairness (e.g. parity) p. 39 ff
- you can analyze an ADM on three levels: complete system, ml model (black box or white box), indvidual prediction
- with complete system they mean data collcetion, pre-processing, feature transformation, model training, post-processing (note: there is actually much more to it: possible feedback loops, manual decision making in the pre-processing, )
- two problems when focusing only on modeling and data is assumed to just be there: data might not be representative, imbalanced data.
- Business understanding and Data understanding come before modelling and are very important.
- We should explain why a decision was made and for which data (subset) the model will not work well. (correct decision yes/no, confidence of model, discrimniation/non-discrimination). Based on which data was the decision made?
- (seems like they dont understand logistic regression well: Ist ihr absoluter Wert sehr klein, so hat das entsprechende Attribut nur wenig bis keinen Einfluss auf die Voraussage. Ist jedoch der
Parameter weit von 0 entfernt (positiv wie negativ), so hat das Attribut entsprechend
Einfluss. -> wrong, depends on scale of feature as well)
- (wrong about decision trees or at least an unprooved statement: Außerdem sind
sie bei Regressionsproblemen (Vorhersage von Zahlenwerten statt binären Entscheidungen)
anderen Modelltypen unterlegen.)
- they recommend explaining the model on an instance level. they don't say why.
- comparison to software testing: unit test, integration test, system test,
- unit test for ADM could be: generate data set with sensitive feature (e.g. gender) and see if decision changes (makes no sense to me, because if it is problematic to have gender in their, you would not have it in the model, so really nonsense.)
- Orakel-Problem: When creating test cases for unit tests, one needs to know what would be the correct output. For that an oracle is needed. This usually has to be a human or can't be done by a human at all maybe.
- meta-morphic testing: instead of testing with absolute values, you just say: for specific instance, when I change feature xj, then prediction has to change by some factor or so
- (their example for meta-morphic testing is meaningless. how would you know by how much the prediction should change when changing the feature? if you would know such things, you wouldn't use machine learning )
- Auditing models
  - code audit: audit a copy of the code and outputs
  - non-invasive audits
  - scraping audits
  - sock puppet audits
  - crowdsource audits
- a platform  has some algo, which affects user. auditing means making a code copy and auditing it
-
CONTINUE: p.66 (non-invasive audits)


## Critique from my side
- not talking enough about the role of the data generating process
- citing LIME only
- No statisticians involved in the creation of the document (let's see if they still get it right, though)
- they dont emphasize problems with the data enough. for example they never speak about that the data might be correct, but reflects a social issue (e.g. preferring to hire men) that should not be encoded by the model.
- dont really talk about cunterfactual explanations, buy only touch on it.
- cite lime instead of other stuff.
- nice that they draw comparisons with unit testing and meta-morphic testing, but i dont think that you can use such things at all, at least not how  they present it. e.g. logistic regression: if you would create a test for how the prediction has to change when i change one feature, then that is exactly the same as saying what weight the feature has. you can test constraints, but once you write a test for something it is fixed (fixed weight for ezample or maybe a monotonicity constraint) and by definition is not longer learned by data and as such you are not "testing" the model. the ultimate test of the model is the generalization error. again, this shows that statistician should be involved.
-

## citation
Gesellschaft für Informatik (2018). Technische und rechtliche Betrachtungen algorithmischer
Entscheidungsverfahren. Studien und Gutachten im Auftrag des Sachverständigenrats für Verbraucherfragen.
Berlin: Sachverständigenrat für Verbraucherfragen.
